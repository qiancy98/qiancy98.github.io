---
layout: post
title:  "AI前景探讨"
---

目录：

- （此行不会显示）
{:toc}


## 写作背景

大概在10月初华为校招那会，我有幸加了个负责智能家居产品线的老师。我们就未来AI产品形态有过一些交流，今记于此。

## 讨论背景

讨论的起源大约是一段洗盘子的视频：机器人不太熟练地从某个指定区域中拿起盘子，洗掉番茄酱，然后把盘子放到碗架上。诚然，这个视频里，机器人动作极为生疏，也会搞出一堆抽象行为，但是这种行为本身在之前就是不可想象的。对于专家系统来说，我需要对每一种盘子制定清洗计划，而盘子种类的丰富程度可以轻易打消我的任何产品化想法：这么看洗盘子应该是某种高价值劳动。但是洗盘子本身却公认为低价值劳动，大家并不承认其劳动中蕴含太多的劳动价值（相比于编程等等）。人类似乎可以在洗完少数盘子后就学会这一行为，并类推到其他各种盘子上。

生成式AI让我们看到了机器化的曙光。相比于以往的专家系统或者特征提取，生成式AI中，完成同样事情所需的样本量看起来少得多（基于我自身观察？虽然需要的样本仍然比人类多）。训练成本的降低则支持了智能驾驶、智能家居等等行业兴起。

## 什么算枯燥的工作

因此我提了这样的问题：什么工作，人们认为枯燥无趣，但是又不得不做？

放到赫胥黎的年代，他大概有个完美回答：电梯操作工。就是站在电梯里，负责操纵电梯在指定楼层停靠的工人。与他们同时代的，还有算盘先生——当然美国没有算盘，他们大概会摆弄些计算尺什么的。在现在，这些工种都只能看到些许残余，比如医院中一些电梯允许工作中的医生比病人优先使用，因而制度上要求电梯操作工存在；东方明珠塔也有电梯操作工，但那是东方明珠塔本身的特殊性质决定的（确实太高了，而且两台轿厢互为配重，需要同步运行）。又或者一些心算比按计算器更快的简单场合，还会保留口算；稍微复杂一点都会下意识拿计算器了。本质上，这些工作高度流程化，可以分解为数个简单步骤，而且需求广泛，从而在70~90年代的电子计算浪潮中，大幅压低了成本。

放到现在，我大概会回答以下几点：
- 小产品线的产品组装工（比如我上次在B站看到个麻将桌的组装流程）；
- 化学实验的过柱子、生物实验的具体实验实施。听说材料学那边筛材料也很痛苦……这我就不了解了。
- 洗盘子、洗菜、择菜。不包括炒菜：炒菜不费什么功夫，而且差一点时间结果差不少，感觉暂时不适合机器。
- 审查数学论文、数学作业的正确性（又或者改任何学科的作业都一样？不清楚。）

我试图寻找这些工作的共性：
- 非标准化。并没有什么标准去明确规定这些工作的步骤、顺序，因此针对每一个产品都要单独设计流程。由于公差存在，也不能写死操作，需要有容错空间。
- 单次量少。麻将桌生产几千张顶天了，一次过柱子也就一两根，做实验每个方案也就几只老鼠。数学作业，一次最多也就一两百份。择菜，一天一家人也吃不了多少。每一件事都并不值得专门为其写大程序，写简单了又必然要处理无数例外，不如人工。
- 长期看，相似工作量多。各种组装工作大同小异；过柱子大同小异无非是检验析出产物有区别；做实验除了那些关键步骤，剩下的也无非是观察；数学作业除了最开始几份费点功夫，后续所有可能证明都记住了，看一眼就知道证明对不对；家常菜的处理大同小异，真有人肉处理不了的也大可以上饭店。但是这些相似点，放在以往就需要每个单独写程序了，并无法抽出如此抽象的共性层。由于单次量少，每次写程序又不划算（写程序意味着自己照着做几遍然后手工抽出其中逻辑，有这时间人工早做完了），最后还是不如人工。

## 当前难点

解决方案则必然是用更少的样本学习：即在某种预处理后，只需要少数样本就可以学习到全部操作，并在保证一定容错前提下执行。样本少、免编程，最好是每次人工展示数遍操作就能学会，那就可以大量普及，而因其通用性导致型号少，因此容易摊平固定研发成本。现在的生成式AI无疑展现了这种可能性。
更好的展望则是机器可以学习各个部分，随后在人工指令下组合这些部分构成整体操作行为，这样就在零样本下学习到了能力。虽然可能，这个梦想稍微远了一点……毕竟即使是人类，也有好多人做不到零样本学习/迁移呢……
（啊，你说让AI自行寻找新的流程、方法论？这种人类都普遍做不好的事情，我可不会指望AI。）

我的认知范围里，除前述问题外，仍存这些难点：
1. 训练外数据的兜底能力。比如说开车遇到复杂路况，应该及时交给人类处理。对于人类离线的情况，也应该保证最坏情况可控（又或者可能现阶段人类不能离线，因为这涉及逻辑判断）。某种意义上说，我们生病时不愿意去小医院，正是因为担心他们不能及时、可靠地将能力外情况转给上级；可以想见AI会遇到同样的信任问题。
2. 与世界交互，并从交互中学习。我始终不相信，在不能交互的情况下，有人能分清相关与因果；因此我也不信AI可以。让AI学会围棋最好的方法不是给它全世界所有的棋谱，而是给它规则让它自我对弈。仿照这个想法，我猜测：让AI学会逻辑最好的方法，不是给它全世界人类的语料，而是给它Lean4让它自己玩玩。但是，如何在训练中设计这样的交互过程呢？
3. 消除人类语料中的额外熵。先用围棋举例：人类棋谱中，难免打勺；让AI模仿人类行为，将本手和打勺一并学了去，就学到了勺带来的额外熵；消除这些熵事实上提升了AI水平。再比如，如果AI看不了棋谱，而只能看什么“靠”、“扳”、“飞”之类的文字叙述，那AI的围棋水平又如何呢？显然此时语言的熵相比于围棋局面本身，就太大了。
4. 自行扩充语料。可以预期，现有语料很快就会不足。下一步应该由AI自行生成语料训练（就像围棋里的自我对弈生成棋谱那样）。可行的思路应该是组合现有逻辑得到某种对世界的输入，使用自身逻辑预测世界会有什么反馈，然后执行以拿到真实反馈，将其和自己的预测比对。如果自己预测可以足够准确，那就可以大量生成语料而不执行，只执行那些自己把握不太大的以进一步修正模型。得到的语料可以用于训练稍小的模型（我不懂AI，这个是不是叫蒸馏来着？）

我最近也用AI写了不少英文数学证明（自然语言证明，不是Lean），我的感受是（下文4点与上方4点一一对应）
1. 哦，数学证明无需兜底，又不是什么人命关天的事情，大不了我接管了手写咯，前面那么多年都没AI，咱不也过来了。
2. AI确实可以猜出我接下来要写什么，但是当它猜错时，通常在逻辑上就错得离谱。这点说，AI仍然是个纯粹的语言模型，缺少和Lean的交互，并因此不知道正确证明为什么正确。
3. 它会纠结于采取哪种文本写法，而非专注于证明本身（指给几种写法分配差不多的选择优先级）。AI会困于语言带来的无效信息熵，而不是逻辑自身的信息熵。
4. AI仍然是模仿为主，需要我或者别人主动演示过然后改写，很难自行组合不同逻辑。（老实说，这点我近年并不奢望。）

我理想中的AI应该可以拿到命题后自行通过发散思维拆解出一系列目标，对每个目标重复拆解，直到这个子目标足够小或者足够熟悉，然后证明之。至于证明过程，我自己的证明更像是图像训练里的扩散模型那样，一开始有个大概，然后不断细化，不断变严谨，直到最后可以写成严谨的数学证明。